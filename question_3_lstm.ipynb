{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: LSTM\n",
    "\n",
    "Following question 1 (2), train a sequence to sequence (e.g., 4 timesteps to 4 timesteps) predictive model (e.g., LSTM)  in the reduced space, and decode predicted results in the full space. Evaluate your algorithm performance on the test dataset using different metrics (e.g., MSE, RMSE, SSIMâ€¦).\n",
    "\n",
    "Reference: https://towardsdatascience.com/multivariate-time-series-forecasting-with-deep-learning-3e7b3e2d2bcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video sequence loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data as sequences\n",
    "import os\n",
    "import cv2 as cv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset to process videos\n",
    "class VideoDataset(Dataset):\n",
    "  def __init__(self, folder_path, sequence_length=4, video_length=16, transform=None):\n",
    "    self.folder_path = folder_path\n",
    "    self.sequence_length = sequence_length\n",
    "    self.video_length = video_length\n",
    "    self.transform = transform\n",
    "    self.video_list = os.listdir(folder_path)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.video_list) * ((self.video_length // self.sequence_length) - 1) # - 1 because we don't want to train on the final sequence - no target sequence!\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    video_idx = idx // (self.sequence_length - 1)\n",
    "    frame_idx = idx % (self.sequence_length - 1)\n",
    "    video_path = os.path.join(self.folder_path, self.video_list[video_idx])\n",
    "    frames = self._load_video(video_path)\n",
    "    input_seq, target_seq = self._get_sequence_pair(frames, frame_idx)\n",
    "    return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_seq, dtype=torch.float32)\n",
    "\n",
    "  def _load_video(self, video_path):\n",
    "    frames = []\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    while True:\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        break\n",
    "      frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "      frames.append(frame.flatten())\n",
    "    cap.release()\n",
    "\n",
    "    frames = np.array(frames) # should be 16 x (128*128) i.e. 16 frames, each 128x128 frame flattened to 1d\n",
    "    return frames\n",
    "\n",
    "  def _get_sequence_pair(self, frames, frame_idx):\n",
    "    input_seq = np.array(frames[frame_idx:frame_idx+self.sequence_length])\n",
    "    target_seq = np.array(frames[frame_idx+self.sequence_length:frame_idx+2 * self.sequence_length])\n",
    "    return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 4\n",
    "folder_path = \"./VIDEOS/training/\"\n",
    "\n",
    "video_dataset = VideoDataset(folder_path, sequence_length=sequence_length, transform=transform)\n",
    "train_loader = DataLoader(video_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the seq2seq model\n",
    "class Seq2SeqModel(nn.Module):\n",
    "  def __init__(self, input_dim, latent_dim, num_layers=1):\n",
    "    super(Seq2SeqModel, self).__init__()\n",
    "\n",
    "    self.encoder = nn.LSTM(input_dim, latent_dim, num_layers, batch_first=True)\n",
    "    self.decoder = nn.LSTM(latent_dim, latent_dim, num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(latent_dim, input_dim)\n",
    "\n",
    "  def forward(self, input_seq):\n",
    "    # Encoder\n",
    "    _, (last_hidden, _) = self.encoder(input_seq)\n",
    "    \n",
    "    # reshape for batch\n",
    "    encoded = last_hidden.repeat(len(input_seq), input_seq.size(1), 1)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_output, _ = self.decoder(encoded)\n",
    "\n",
    "    output = self.fc(decoder_output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use GPU if available\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq model training:\n",
    "\n",
    "This LSTM autoencoder will be trained on the full image input (128x128) flattened to a 16,384-element vector. This is not good! Later, will experiment with reduced input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to train the model\n",
    "def train(model, train_data, criterion, optimizer, epochs):\n",
    "  model.train\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for input_seq, target_seq in train_data:\n",
    "      input_seq, target_seq = input_seq.to(device, dtype=torch.float32), target_seq.to(device, dtype=torch.float32)\n",
    "      optimizer.zero_grad()\n",
    "      output_seq = model(input_seq)\n",
    "      loss = criterion(output_seq, target_seq)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "input_dim = 128*128\n",
    "latent_dim = 512\n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model, loss function, and optimizer\n",
    "model = Seq2SeqModel(input_dim, latent_dim, num_layers)\n",
    "model = model.to(device=device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on train set\n",
    "\n",
    "To perform initial evaluations, I will load in the sequences of one video, stored in ./VIDEOS/small_test/, to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './VIDEOS/small_test/'\n",
    "test_dataset = VideoDataset(folder_path, sequence_length=sequence_length, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(predictions):\n",
    "  return predictions.view(batch_size, sequence_length, 128, 128)  # Reshape to image dimensions\n",
    "\n",
    "def visualise_one_test_video(model, test_loader):\n",
    "  model.eval()\n",
    "  for input_seq, target_seq in test_loader:\n",
    "    with torch.no_grad():\n",
    "      input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "      # Predict sequences in reduced space\n",
    "      output_seq = model(input_seq)\n",
    "      # Decode predicted results into full space\n",
    "      decoded_predictions = decode_predictions(output_seq)\n",
    "      decoded_targets = decode_predictions(target_seq)\n",
    "      \n",
    "      # Example: Convert the first sequence in the batch to an image (assuming grayscale)\n",
    "      \n",
    "      fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "      for i in range(sequence_length):\n",
    "        example_prediction = decoded_predictions[0, i].cpu().numpy()  # Extract first prediction from batch\n",
    "        example_prediction = np.uint8(example_prediction * 255)  # Convert to 8-bit grayscale\n",
    "\n",
    "        example_target = decoded_targets[0, i].cpu().numpy()\n",
    "        example_target = np.uint8(example_target * 255)\n",
    "\n",
    "        ax[0, i].imshow(example_prediction, cmap='binary')\n",
    "        ax[1, i].imshow(example_target, cmap='binary')\n",
    "\n",
    "      plt.show()\n",
    "\n",
    "visualise_one_test_video(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on reduced-dimension inputs\n",
    "\n",
    "Clearly, passing in the flattened full image is not good. Instead, I will pass in the input images to the CAE, perform the LSTM encode/decode process using the encoded result of the CAE, and then decode the output of the LSTM decode process using the CAE decoder, and compute reconstruction accuracy that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CAE_LSTM model\n",
    "class CAE_LSTM(nn.Module):\n",
    "  def __init__(self, input_dim, cae_latent_dim, lstm_latent_dim, num_layers=1):\n",
    "    super(CAE_LSTM, self).__init__()\n",
    "\n",
    "    # CAE encoder\n",
    "    self.cae_encoder = nn.Sequential(\n",
    "      nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      nn.Conv2d(16, cae_latent_dim, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    )\n",
    "\n",
    "    # CAE decoder\n",
    "    self.cae_decoder = nn.Sequential(\n",
    "      nn.ConvTranspose2d(cae_latent_dim, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    # LSTM encoder\n",
    "    self.lstm_encoder = nn.LSTM(input_dim, lstm_latent_dim, num_layers, batch_first=True)\n",
    "\n",
    "    # LSTM decoder\n",
    "    self.lstm_decoder = nn.LSTM(lstm_latent_dim, lstm_latent_dim, num_layers, batch_first=True)\n",
    "\n",
    "    # Fully-connected layer\n",
    "    self.fc = nn.Linear(lstm_latent_dim, input_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # obtain the encoded form of the input sequence\n",
    "    x = self.cae_encoder(x)\n",
    "\n",
    "    # run the encoded input through the LSTM network\n",
    "    _, (last_hidden, _) = self.lstm_encoder(x)\n",
    "    encoded = last_hidden.repeat(len(x), x.size(1), 1)\n",
    "\n",
    "    # get the predictions from the LSTM\n",
    "    decoder_output, _ = self.lstm_decoder(encoded)\n",
    "    output = self.fc(decoder_output)\n",
    "\n",
    "    # obtain the decoded images\n",
    "    output = self.cae_decoder(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cae_latent_dim = 64\n",
    "lstm_latent_dim = 512\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialise model, loss function, and optimizer\n",
    "model = CAE_LSTM(input_dim, cae_latent_dim, lstm_latent_dim, num_layers)\n",
    "model = model.to(device=device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
